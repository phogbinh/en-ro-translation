{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ebeb00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Helsinki-NLP/opus-mt-en-ro\"\n",
    "MAX_INPUT_LENGTH = 128\n",
    "SOURCE_LANG = \"en\"\n",
    "TARGET_LANG = \"ro\"\n",
    "MODEL_OUTPUT_DIR = \"trained_model/\"\n",
    "MODEL_EVALUATION_STRATEGY = \"epoch\"\n",
    "MODEL_LEARNING_RATE = 2e-5\n",
    "MODEL_BATCH_SIZE = 16\n",
    "MODEL_WEIGHT_DECAY = 0.01\n",
    "MODEL_SAVE_TOTAL_LIMIT = 3\n",
    "MODEL_EPOCHS_NUM = 1\n",
    "MODEL_PREDICT_WITH_GENERATE = True\n",
    "MODEL_FP16 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f2975f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba4662ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wmt16 (/home/bill/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70fba071fcfb417baccfccf4f98ba326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 610320\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1999\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1999\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"wmt16\", \"ro-en\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "487b4704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3618116/3377329550.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"sacrebleu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Metric(name: \"sacrebleu\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, usage: \"\"\"\n",
       "Produces BLEU scores along with its sufficient statistics\n",
       "from a source against one or more references.\n",
       "\n",
       "Args:\n",
       "    predictions (`list` of `str`): list of translations to score. Each translation should be tokenized into a list of tokens.\n",
       "    references (`list` of `list` of `str`): A list of lists of references. The contents of the first sub-list are the references for the first prediction, the contents of the second sub-list are for the second prediction, etc. Note that there must be the same number of references for each prediction (i.e. all sub-lists must be of the same length).\n",
       "    smooth_method (`str`): The smoothing method to use, defaults to `'exp'`. Possible values are:\n",
       "        - `'none'`: no smoothing\n",
       "        - `'floor'`: increment zero counts\n",
       "        - `'add-k'`: increment num/denom by k for n>1\n",
       "        - `'exp'`: exponential decay\n",
       "    smooth_value (`float`): The smoothing value. Only valid when `smooth_method='floor'` (in which case `smooth_value` defaults to `0.1`) or `smooth_method='add-k'` (in which case `smooth_value` defaults to `1`).\n",
       "    tokenize (`str`): Tokenization method to use for BLEU. If not provided, defaults to `'zh'` for Chinese, `'ja-mecab'` for Japanese and `'13a'` (mteval) otherwise. Possible values are:\n",
       "        - `'none'`: No tokenization.\n",
       "        - `'zh'`: Chinese tokenization.\n",
       "        - `'13a'`: mimics the `mteval-v13a` script from Moses.\n",
       "        - `'intl'`: International tokenization, mimics the `mteval-v14` script from Moses\n",
       "        - `'char'`: Language-agnostic character-level tokenization.\n",
       "        - `'ja-mecab'`: Japanese tokenization. Uses the [MeCab tokenizer](https://pypi.org/project/mecab-python3).\n",
       "    lowercase (`bool`): If `True`, lowercases the input, enabling case-insensitivity. Defaults to `False`.\n",
       "    force (`bool`): If `True`, insists that your tokenized input is actually detokenized. Defaults to `False`.\n",
       "    use_effective_order (`bool`): If `True`, stops including n-gram orders for which precision is 0. This should be `True`, if sentence-level BLEU will be computed. Defaults to `False`.\n",
       "\n",
       "Returns:\n",
       "    'score': BLEU score,\n",
       "    'counts': Counts,\n",
       "    'totals': Totals,\n",
       "    'precisions': Precisions,\n",
       "    'bp': Brevity penalty,\n",
       "    'sys_len': predictions length,\n",
       "    'ref_len': reference length,\n",
       "\n",
       "Examples:\n",
       "\n",
       "    Example 1:\n",
       "        >>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
       "        >>> references = [[\"hello there general kenobi\", \"hello there !\"], [\"foo bar foobar\", \"foo bar foobar\"]]\n",
       "        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n",
       "        >>> results = sacrebleu.compute(predictions=predictions, references=references)\n",
       "        >>> print(list(results.keys()))\n",
       "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
       "        >>> print(round(results[\"score\"], 1))\n",
       "        100.0\n",
       "\n",
       "    Example 2:\n",
       "        >>> predictions = [\"hello there general kenobi\",\n",
       "        ...                 \"on our way to ankh morpork\"]\n",
       "        >>> references = [[\"hello there general kenobi\", \"hello there !\"],\n",
       "        ...                 [\"goodbye ankh morpork\", \"ankh morpork\"]]\n",
       "        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n",
       "        >>> results = sacrebleu.compute(predictions=predictions,\n",
       "        ...                             references=references)\n",
       "        >>> print(list(results.keys()))\n",
       "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
       "        >>> print(round(results[\"score\"], 1))\n",
       "        39.8\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"sacrebleu\")\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d957e6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bill/miniconda3/envs/testenv/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [125, 778, 3, 63, 141, 9191, 23, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if \"mbart\" in MODEL_NAME:\n",
    "  tokenizer.src_lang = \"en-XX\"\n",
    "  tokenizer.tgt_lang = \"ro-RO\"\n",
    "tokenizer(\"Hello, this one sentence!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d418b72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if MODEL_NAME in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "  prefix = \"translate English to Romanian: \"\n",
    "else:\n",
    "  prefix = \"\"\n",
    "prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd1c902e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bill/miniconda3/envs/testenv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[393, 4462, 14, 1137, 53, 216, 28636, 0], [24385, 14, 28636, 14, 4646, 4622, 53, 216, 28636, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[42140, 494, 1750, 53, 8, 59, 903, 3543, 9, 15202, 0], [36199, 6612, 9, 15202, 122, 568, 35788, 21549, 53, 8, 59, 903, 3543, 9, 15202, 0]]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process(examples):\n",
    "  inputs = [prefix + example[SOURCE_LANG] for example in examples[\"translation\"]]\n",
    "  targets = [example[TARGET_LANG] for example in examples[\"translation\"]]\n",
    "  model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n",
    "  with tokenizer.as_target_tokenizer():\n",
    "    labels = tokenizer(targets, max_length=MAX_INPUT_LENGTH, truncation=True)\n",
    "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "  return model_inputs\n",
    "\n",
    "process(raw_datasets[\"train\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cce3ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/bill/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227/cache-89702f57dec378f3.arrow\n",
      "Loading cached processed dataset at /home/bill/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227/cache-af72b3f5a0423bd8.arrow\n",
      "Loading cached processed dataset at /home/bill/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227/cache-820db1c98dd6aaf0.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 610320\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1999\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1999\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(process, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8018f1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89c7b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "  output_dir=MODEL_OUTPUT_DIR,\n",
    "  evaluation_strategy=MODEL_EVALUATION_STRATEGY,\n",
    "  learning_rate=MODEL_LEARNING_RATE,\n",
    "  per_device_train_batch_size=MODEL_BATCH_SIZE,\n",
    "  per_device_eval_batch_size=MODEL_BATCH_SIZE,\n",
    "  weight_decay=MODEL_WEIGHT_DECAY,\n",
    "  save_total_limit=MODEL_SAVE_TOTAL_LIMIT,\n",
    "  num_train_epochs=MODEL_EPOCHS_NUM,\n",
    "  predict_with_generate=MODEL_PREDICT_WITH_GENERATE,\n",
    "  fp16=MODEL_FP16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da61811f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForSeq2Seq(tokenizer=PreTrainedTokenizer(name_or_path='Helsinki-NLP/opus-mt-en-ro', vocab_size=59543, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}), model=MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(59543, 512, padding_idx=59542)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(59543, 512, padding_idx=59542)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(59543, 512, padding_idx=59542)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=59543, bias=False)\n",
       "), padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08957065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "  predictions = [prediction.strip() for prediction in predictions]\n",
    "  labels = [[label.strip()] for label in labels]\n",
    "  return predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c780059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(evaluation_predictions):\n",
    "  predictions, labels = evaluation_predictions\n",
    "  if isinstance(predictions, tuple):\n",
    "    predictions = predictions[0]\n",
    "  decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "  decoded_predictions, decoded_labels = postprocess(decoded_predictions, decoded_labels)\n",
    "  result = metric.compute(predictions=decoded_predictions, references=decoded_labels)\n",
    "  result = {\"bleu\": result[\"score\"]}\n",
    "  prediction_lengths = [np.count_nonzero(prediction != tokenizer.pad_token_id) for prediction in predictions]\n",
    "  result[\"get_len\"] = np.mean(prediction_lengths)\n",
    "  result = {key: round(value, 4) for key, value in result.items()}\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd9de7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "  model,\n",
    "  training_args,\n",
    "  train_dataset=tokenized_datasets[\"train\"],\n",
    "  eval_dataset=tokenized_datasets[\"validation\"],\n",
    "  data_collator=data_collator,\n",
    "  tokenizer=tokenizer,\n",
    "  compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bd0bd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation. If translation are not expected by `MarianMTModel.forward`,  you can safely ignore this message.\n",
      "/home/bill/miniconda3/envs/testenv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 610320\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 38145\n",
      "  Number of trainable parameters = 74624512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38145' max='38145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38145/38145 1:23:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Get Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.744100</td>\n",
       "      <td>1.288951</td>\n",
       "      <td>28.162100</td>\n",
       "      <td>34.084000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to trained_model/checkpoint-500\n",
      "Configuration saved in trained_model/checkpoint-500/config.json\n",
      "Model weights saved in trained_model/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-37000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-1000\n",
      "Configuration saved in trained_model/checkpoint-1000/config.json\n",
      "Model weights saved in trained_model/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-37500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-1500\n",
      "Configuration saved in trained_model/checkpoint-1500/config.json\n",
      "Model weights saved in trained_model/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-38000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-2000\n",
      "Configuration saved in trained_model/checkpoint-2000/config.json\n",
      "Model weights saved in trained_model/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-2500\n",
      "Configuration saved in trained_model/checkpoint-2500/config.json\n",
      "Model weights saved in trained_model/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-3000\n",
      "Configuration saved in trained_model/checkpoint-3000/config.json\n",
      "Model weights saved in trained_model/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-3500\n",
      "Configuration saved in trained_model/checkpoint-3500/config.json\n",
      "Model weights saved in trained_model/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-4000\n",
      "Configuration saved in trained_model/checkpoint-4000/config.json\n",
      "Model weights saved in trained_model/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-4500\n",
      "Configuration saved in trained_model/checkpoint-4500/config.json\n",
      "Model weights saved in trained_model/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-5000\n",
      "Configuration saved in trained_model/checkpoint-5000/config.json\n",
      "Model weights saved in trained_model/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-5500\n",
      "Configuration saved in trained_model/checkpoint-5500/config.json\n",
      "Model weights saved in trained_model/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-6000\n",
      "Configuration saved in trained_model/checkpoint-6000/config.json\n",
      "Model weights saved in trained_model/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-6500\n",
      "Configuration saved in trained_model/checkpoint-6500/config.json\n",
      "Model weights saved in trained_model/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-7000\n",
      "Configuration saved in trained_model/checkpoint-7000/config.json\n",
      "Model weights saved in trained_model/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-7500\n",
      "Configuration saved in trained_model/checkpoint-7500/config.json\n",
      "Model weights saved in trained_model/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-8000\n",
      "Configuration saved in trained_model/checkpoint-8000/config.json\n",
      "Model weights saved in trained_model/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-8500\n",
      "Configuration saved in trained_model/checkpoint-8500/config.json\n",
      "Model weights saved in trained_model/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-9000\n",
      "Configuration saved in trained_model/checkpoint-9000/config.json\n",
      "Model weights saved in trained_model/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-9500\n",
      "Configuration saved in trained_model/checkpoint-9500/config.json\n",
      "Model weights saved in trained_model/checkpoint-9500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in trained_model/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-10000\n",
      "Configuration saved in trained_model/checkpoint-10000/config.json\n",
      "Model weights saved in trained_model/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-10500\n",
      "Configuration saved in trained_model/checkpoint-10500/config.json\n",
      "Model weights saved in trained_model/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-11000\n",
      "Configuration saved in trained_model/checkpoint-11000/config.json\n",
      "Model weights saved in trained_model/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-11500\n",
      "Configuration saved in trained_model/checkpoint-11500/config.json\n",
      "Model weights saved in trained_model/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-11500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-12000\n",
      "Configuration saved in trained_model/checkpoint-12000/config.json\n",
      "Model weights saved in trained_model/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-12000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-12500\n",
      "Configuration saved in trained_model/checkpoint-12500/config.json\n",
      "Model weights saved in trained_model/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-12500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-13000\n",
      "Configuration saved in trained_model/checkpoint-13000/config.json\n",
      "Model weights saved in trained_model/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-13000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-13500\n",
      "Configuration saved in trained_model/checkpoint-13500/config.json\n",
      "Model weights saved in trained_model/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-13500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-12000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-14000\n",
      "Configuration saved in trained_model/checkpoint-14000/config.json\n",
      "Model weights saved in trained_model/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-14000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-14500\n",
      "Configuration saved in trained_model/checkpoint-14500/config.json\n",
      "Model weights saved in trained_model/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-14500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-15000\n",
      "Configuration saved in trained_model/checkpoint-15000/config.json\n",
      "Model weights saved in trained_model/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-15000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-15500\n",
      "Configuration saved in trained_model/checkpoint-15500/config.json\n",
      "Model weights saved in trained_model/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-15500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-14000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-16000\n",
      "Configuration saved in trained_model/checkpoint-16000/config.json\n",
      "Model weights saved in trained_model/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-16000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-16500\n",
      "Configuration saved in trained_model/checkpoint-16500/config.json\n",
      "Model weights saved in trained_model/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-16500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-15000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-17000\n",
      "Configuration saved in trained_model/checkpoint-17000/config.json\n",
      "Model weights saved in trained_model/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-17000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-17500\n",
      "Configuration saved in trained_model/checkpoint-17500/config.json\n",
      "Model weights saved in trained_model/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-17500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-16000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-18000\n",
      "Configuration saved in trained_model/checkpoint-18000/config.json\n",
      "Model weights saved in trained_model/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-18000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-18500\n",
      "Configuration saved in trained_model/checkpoint-18500/config.json\n",
      "Model weights saved in trained_model/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-18500/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in trained_model/checkpoint-18500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-17000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-19000\n",
      "Configuration saved in trained_model/checkpoint-19000/config.json\n",
      "Model weights saved in trained_model/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-19000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-19500\n",
      "Configuration saved in trained_model/checkpoint-19500/config.json\n",
      "Model weights saved in trained_model/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-19500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-18000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-20000\n",
      "Configuration saved in trained_model/checkpoint-20000/config.json\n",
      "Model weights saved in trained_model/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-20000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-20500\n",
      "Configuration saved in trained_model/checkpoint-20500/config.json\n",
      "Model weights saved in trained_model/checkpoint-20500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-20500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-19000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-21000\n",
      "Configuration saved in trained_model/checkpoint-21000/config.json\n",
      "Model weights saved in trained_model/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-21000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-21500\n",
      "Configuration saved in trained_model/checkpoint-21500/config.json\n",
      "Model weights saved in trained_model/checkpoint-21500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-21500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-20000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-22000\n",
      "Configuration saved in trained_model/checkpoint-22000/config.json\n",
      "Model weights saved in trained_model/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-22000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-22500\n",
      "Configuration saved in trained_model/checkpoint-22500/config.json\n",
      "Model weights saved in trained_model/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-22500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-21000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-23000\n",
      "Configuration saved in trained_model/checkpoint-23000/config.json\n",
      "Model weights saved in trained_model/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-23000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-23500\n",
      "Configuration saved in trained_model/checkpoint-23500/config.json\n",
      "Model weights saved in trained_model/checkpoint-23500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-23500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-23500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-22000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-24000\n",
      "Configuration saved in trained_model/checkpoint-24000/config.json\n",
      "Model weights saved in trained_model/checkpoint-24000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-24000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-24500\n",
      "Configuration saved in trained_model/checkpoint-24500/config.json\n",
      "Model weights saved in trained_model/checkpoint-24500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-24500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-24500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-23000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-25000\n",
      "Configuration saved in trained_model/checkpoint-25000/config.json\n",
      "Model weights saved in trained_model/checkpoint-25000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-25000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-25500\n",
      "Configuration saved in trained_model/checkpoint-25500/config.json\n",
      "Model weights saved in trained_model/checkpoint-25500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-25500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-25500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-24000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-26000\n",
      "Configuration saved in trained_model/checkpoint-26000/config.json\n",
      "Model weights saved in trained_model/checkpoint-26000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-26000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-26000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-26500\n",
      "Configuration saved in trained_model/checkpoint-26500/config.json\n",
      "Model weights saved in trained_model/checkpoint-26500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-26500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-26500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-25000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-27000\n",
      "Configuration saved in trained_model/checkpoint-27000/config.json\n",
      "Model weights saved in trained_model/checkpoint-27000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-27000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-27000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-27500\n",
      "Configuration saved in trained_model/checkpoint-27500/config.json\n",
      "Model weights saved in trained_model/checkpoint-27500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-27500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-27500/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [trained_model/checkpoint-26000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-28000\n",
      "Configuration saved in trained_model/checkpoint-28000/config.json\n",
      "Model weights saved in trained_model/checkpoint-28000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-28000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-28000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-28500\n",
      "Configuration saved in trained_model/checkpoint-28500/config.json\n",
      "Model weights saved in trained_model/checkpoint-28500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-28500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-28500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-27000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-29000\n",
      "Configuration saved in trained_model/checkpoint-29000/config.json\n",
      "Model weights saved in trained_model/checkpoint-29000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-29000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-29000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-29500\n",
      "Configuration saved in trained_model/checkpoint-29500/config.json\n",
      "Model weights saved in trained_model/checkpoint-29500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-29500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-29500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-28000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-30000\n",
      "Configuration saved in trained_model/checkpoint-30000/config.json\n",
      "Model weights saved in trained_model/checkpoint-30000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-30000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-30500\n",
      "Configuration saved in trained_model/checkpoint-30500/config.json\n",
      "Model weights saved in trained_model/checkpoint-30500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-30500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-30500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-29000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-31000\n",
      "Configuration saved in trained_model/checkpoint-31000/config.json\n",
      "Model weights saved in trained_model/checkpoint-31000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-31000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-31000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-31500\n",
      "Configuration saved in trained_model/checkpoint-31500/config.json\n",
      "Model weights saved in trained_model/checkpoint-31500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-31500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-31500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-30000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-32000\n",
      "Configuration saved in trained_model/checkpoint-32000/config.json\n",
      "Model weights saved in trained_model/checkpoint-32000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-32000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-32500\n",
      "Configuration saved in trained_model/checkpoint-32500/config.json\n",
      "Model weights saved in trained_model/checkpoint-32500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-32500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-32500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-31000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-33000\n",
      "Configuration saved in trained_model/checkpoint-33000/config.json\n",
      "Model weights saved in trained_model/checkpoint-33000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-33000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-33000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-33500\n",
      "Configuration saved in trained_model/checkpoint-33500/config.json\n",
      "Model weights saved in trained_model/checkpoint-33500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-33500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-33500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-32000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-34000\n",
      "Configuration saved in trained_model/checkpoint-34000/config.json\n",
      "Model weights saved in trained_model/checkpoint-34000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-34000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-34000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-34500\n",
      "Configuration saved in trained_model/checkpoint-34500/config.json\n",
      "Model weights saved in trained_model/checkpoint-34500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-34500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-34500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-33000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-35000\n",
      "Configuration saved in trained_model/checkpoint-35000/config.json\n",
      "Model weights saved in trained_model/checkpoint-35000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-35000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-35000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-35500\n",
      "Configuration saved in trained_model/checkpoint-35500/config.json\n",
      "Model weights saved in trained_model/checkpoint-35500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-35500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-35500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-34000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-36000\n",
      "Configuration saved in trained_model/checkpoint-36000/config.json\n",
      "Model weights saved in trained_model/checkpoint-36000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-36000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-36000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-34500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-36500\n",
      "Configuration saved in trained_model/checkpoint-36500/config.json\n",
      "Model weights saved in trained_model/checkpoint-36500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-36500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-36500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-35000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to trained_model/checkpoint-37000\n",
      "Configuration saved in trained_model/checkpoint-37000/config.json\n",
      "Model weights saved in trained_model/checkpoint-37000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-37000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-37000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-35500] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-37500\n",
      "Configuration saved in trained_model/checkpoint-37500/config.json\n",
      "Model weights saved in trained_model/checkpoint-37500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-37500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-37500/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-36000] due to args.save_total_limit\n",
      "Saving model checkpoint to trained_model/checkpoint-38000\n",
      "Configuration saved in trained_model/checkpoint-38000/config.json\n",
      "Model weights saved in trained_model/checkpoint-38000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-38000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-38000/special_tokens_map.json\n",
      "Deleting older checkpoint [trained_model/checkpoint-36500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation. If translation are not expected by `MarianMTModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1999\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model = True\n",
    "if train_model:\n",
    "  trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dfb3f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in trained_model/config.json\n",
      "Model weights saved in trained_model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "save_model = True\n",
    "if save_model:\n",
    "  model.save_pretrained(MODEL_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50688230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file trained_model/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"trained_model/\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      59542\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 59542,\n",
      "  \"decoder_vocab_size\": 59543,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 59542,\n",
      "  \"scale_embedding\": true,\n",
      "  \"share_encoder_decoder_embeddings\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 59543\n",
      "}\n",
      "\n",
      "loading weights file trained_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at trained_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "trained_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b56c261a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am a French colonist.', 'I am an American asshole.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts = [\"I am a French colonist.\", \"I am an American asshole.\"]\n",
    "test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db054ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bill/miniconda3/envs/testenv/lib/python3.8/site-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[59542,   276, 16018,   869, 11479,     2,     0, 59542, 59542],\n",
       "        [59542,   276,    42,    88,  3795,  2491,  6437,     2,     0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = tokenizer(test_texts, padding=True, return_tensors=\"pt\")\n",
    "model_output = trained_model.generate(**model_input, num_beams=5)\n",
    "model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5396882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sunt colonist francez.', 'Sunt un dobitoc american.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(model_output, skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
